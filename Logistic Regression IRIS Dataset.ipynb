{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title: Logistic Regression Implementation\n",
    ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IRIS Dataset - Using the characterstics/Dimensions of the plant we want to identify/predict which plant species are we\n",
    "## talking about\n",
    "## Sepal length, Sepal width, Petal length and Petal width\n",
    "\n",
    "#############\n",
    "## DATA IO ##\n",
    "#############\n",
    "\n",
    "def get_data(filepath):\n",
    "    # Opens the file handler for the dataset file. Using variable 'f' we can access and manipulate our file anywhere \n",
    "    # in our code after the next code line.\n",
    "    f = open(filepath,\"r\")\n",
    "\n",
    "    # Predictors Collection (or your input variable) \n",
    "    # (which in this case is the Sepal length, Sepal width, Petal length and Petal width)\n",
    "    X = [[],[],[],[]]\n",
    "\n",
    "    # Output Response (or your output variable) (which in this case is the category to which the iris flower belongs.\n",
    "    # Remember: We are interested in seperating Setosa category [label - 1] from other non-setosa categories [label - 0].)\n",
    "    Y = []\n",
    "\n",
    "    # Initializing a reader generator using reader method from csv module. A reader generator takes each line from the \n",
    "    # file and converts it into list of columns.\n",
    "    reader = csv.reader(f)\n",
    "\n",
    "    # Using for loop, we are able to read one row at a time.\n",
    "    for row in reader:\n",
    "        # For extracting out 4 input variables i.e Sepal length, Sepal width, Petal length, Petal width\n",
    "        for i in range(0,4):\n",
    "            X[i].append(float(row[i]))\n",
    "        \n",
    "        # For extracting output category (Y) i.e 1 for Setosa and 0 for Non-Setosa\n",
    "        if row[4] == \"Iris-setosa\":\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            Y.append(0)\n",
    "           \n",
    "\n",
    "    # Close the file once we have succesffuly stored all data into our X and Y variables.\n",
    "    f.close()\n",
    "    \n",
    "    # Normalization of Input Data.\n",
    "    mean = []\n",
    "    std = []\n",
    "    \n",
    "    for i in range(0,4):\n",
    "        X[i] = np.array(X[i])\n",
    "        mean.append(np.mean(X[i]))\n",
    "        std.append(np.std(X[i]))\n",
    "    \n",
    "    # Returning Normalized Input Data \n",
    "    return [np.array([(X[i] - mean[i])/(std[i]) for i in range(0,4)]),np.array(Y),mean,std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of logistic function\n",
    "# a = beta_0 + (beta_1 * X1) + (beta_2 * X2) + ......\n",
    "def logistic(a):\n",
    "    return (1/(1+np.exp(-1*a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "## Error Calculation ##\n",
    "#####################\n",
    "\n",
    "\n",
    "# Error Function to calculate value of the cost function [ -Y(log(_y)) - ((1-Y)*log(1-(_y))) ].\n",
    "# _y -> Predicted Value \n",
    "# Y -> Actual Value\n",
    "def error(x, y, betas):\n",
    "    error = 0\n",
    "    for i in range(x[0].shape[0]):\n",
    "        predicted_value = logistic(betas[0] + (betas[1] * x[0][i]) + (betas[2] * x[1][i]) + (betas[3] * x[2][i]) + (betas[4] * x[3][i]))\n",
    "        actual_value = y[i]\n",
    "        error = error + (((-1)*(actual_value)*np.log(predicted_value)) - ((1-actual_value)*np.log(1-predicted_value)))\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Value using logistic function for a given input combination\n",
    "def predicted_value_for_ithRow(X,i,betas):\n",
    "    return (logistic(betas[0] + (betas[1] * X[0][i]) + (betas[2] * X[1][i]) + (betas[3] * X[2][i]) + (betas[4] * X[3][i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent algorithm implementation\n",
    "def gradientDescentAlgorithm(x, y, learning_rate):\n",
    "    \n",
    "    print (\"Training Linear Regression Model using Gradient Descent\")\n",
    "    \n",
    "    maximum_iterations = 100\n",
    "    \n",
    "    # This flag lets the program know wether the gradient descent algorithm has reached it's converged state which means wether \n",
    "    # the algorithm was able to find the local minima (where the slope of cost function wrt your parameters [betas] is zero)\n",
    "    converge_status = False\n",
    "    \n",
    "    # num_rows stores the number of datapoints in the current dataset provided for training.\n",
    "    num_rows = x[0].shape[0]\n",
    "\n",
    "    # Initial Value of parameters\n",
    "    betas = [0,0,0,0,0]\n",
    "\n",
    "    # Initial Error or Cost based on the initial parameter values\n",
    "    _error = error(x, y, betas)\n",
    "    \n",
    "    # Iterate Loop\n",
    "    num_iter = 0\n",
    "    while not converge_status:\n",
    "        # for each training sample, compute the gradient (d/d_beta j(beta))\n",
    "        gradient_0 = 1.0/num_rows * sum([(predicted_value_for_ithRow(x,i,betas) - y[i]) for i in range(num_rows)]) \n",
    "        gradient_1 = 1.0/num_rows * sum([(predicted_value_for_ithRow(x,i,betas) - y[i])*x[0][i] for i in range(num_rows)])\n",
    "        gradient_2 = 1.0/num_rows * sum([(predicted_value_for_ithRow(x,i,betas) - y[i])*x[1][i] for i in range(num_rows)])\n",
    "        gradient_3 = 1.0/num_rows * sum([(predicted_value_for_ithRow(x,i,betas) - y[i])*x[2][i] for i in range(num_rows)])\n",
    "        gradient_4 = 1.0/num_rows * sum([(predicted_value_for_ithRow(x,i,betas) - y[i])*x[3][i] for i in range(num_rows)])\n",
    "        \n",
    "        # Computation of new parameters according to the current gradient.\n",
    "        temp0 = betas[0] - learning_rate * gradient_0\n",
    "        temp1 = betas[1] - learning_rate * gradient_1\n",
    "        temp2 = betas[2] - learning_rate * gradient_2\n",
    "        temp3 = betas[3] - learning_rate * gradient_3\n",
    "        temp4 = betas[4] - learning_rate * gradient_4\n",
    "        \n",
    "    \n",
    "        # Simultaneous Update of Parameters betas.\n",
    "        betas[0] = temp0\n",
    "        betas[1] = temp1\n",
    "        betas[2] = temp2\n",
    "        betas[3] = temp3\n",
    "        betas[4] = temp4\n",
    "\n",
    "        current_error = error(x, y, betas)\n",
    "        \n",
    "        if num_iter % 10 == 0:\n",
    "            print ('Iteration',num_iter+1,'Current Value of Error (Cost Function) based on updated values of beta parameters = ', current_error)\n",
    "            \n",
    "        _error = current_error   # update error \n",
    "        num_iter = num_iter + 1  # update iter\n",
    "    \n",
    "        if num_iter == maximum_iterations:\n",
    "            print (\"Training Interrupted as Maximum number of iterations were crossed.\\n\\n\")\n",
    "            converge_status = True\n",
    "\n",
    "    return (betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to predict response variable Y for new values of X  using the estimated coefficients.\n",
    "# This method can predict Response variable (Y) for single as well as multiple values of X. If only a single numerical Value\n",
    "# input variable (X). It will return the prediction for only that single numerical\n",
    "# value. If a collection of different values for input variable (list) is passed, it will return a list of predictions\n",
    "# for each input value.\n",
    "# \"if\" statement on line number 155 takes care of understanding if the input value is singular or a list.\n",
    "def predict_probability(coef,X):\n",
    "\tbeta_0 = coef[0]\n",
    "\tbeta_1 = coef[1]\n",
    "\tbeta_2 = coef[2]\n",
    "\tbeta_3 = coef[3]\n",
    "\tbeta_4 = coef[4]\n",
    "    \n",
    "\tfy = []\n",
    "\tif len(X) > 1:\n",
    "\t\tfor x in X:\n",
    "\t\t\ta = beta_0 + (beta_1 * x[0]) + (beta_2 * x[1]) + (beta_3 * x[2]) + (beta_4 * x[3])\n",
    "\t\t\tresult = logistic(a)\n",
    "\t\t\tprint(x[0],x[1],x[2],x[3],a,result)\n",
    "\t\t\tfy.append(result)\n",
    "\t\treturn fy\n",
    "\n",
    "\t# Our Regression Model defined using the coefficients from slr function\n",
    "\tx = X[0]\n",
    "\tY = logistic(beta_0 + (beta_1 * x[0]) + (beta_2 * x[1]) + (beta_3 * x[2]) + (beta_4 * x[3]))\n",
    "\n",
    "\treturn Y\n",
    "# Predicted Probability that the flower is Setosa (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Predicted Probability using logistic function and coefficients, we classify the unknown data point into \n",
    "# one of two categories using threshold.\n",
    "def classify(predicted_probability,threshold = 0.5):\n",
    "    if predicted_probability > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Actual Category and Predicted Category we compute the components of confusion matrix i.e\n",
    "# TP - True Positive [Model - Setosa, Actual - Setosa]\n",
    "# FP - False Positive [Model - Setosa, Actual - Non Setosa]\n",
    "# TN - True Negative [Model - Non Setosa, Actual - Non Setosa]\n",
    "# FN - False Negative [Model - Non Setosa, Actual - Setosa]\n",
    "def get_confusion_matrix(test_Y,pred_Y):\n",
    "    tp = (pred_Y[test_Y == 1])\n",
    "    tp = tp[tp==1]\n",
    "    tp = tp.shape[0]\n",
    "    \n",
    "    tn = (pred_Y[test_Y == 0])\n",
    "    tn = tn[tn==0]\n",
    "    tn = tn.shape[0]\n",
    "    \n",
    "    fn = 90 - tn\n",
    "    fp = 45 - tp\n",
    "    \n",
    "    return [tp,fp,tn,fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on components of confusion matrix, we compute accuracy, precision and recall evaluation metrics.\n",
    "def evaluate_performance(test_Y,pred_Y):\n",
    "    tp,fp,tn,fn = get_confusion_matrix(test_Y,pred_Y)\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    \n",
    "    # Precision of Positive class = (TP / TP + FP) \n",
    "    # Similarly precision of negative class = (TN / TN + FN)\n",
    "    # Precision = Avg (Precision of Positive Class, Precision of Negative Class)\n",
    "    precision = ((tp / (tp + fp)) + (tn / (tn + fn)))/2\n",
    "    \n",
    "    # Recall of Positive class = (TP / TP + FN) \n",
    "    # Similarly Recall of Negative class = (TN / TN + FP)\n",
    "    # Recall = Avg (Recall of Positive Class, Recall of Negative Class)\n",
    "    recall = ((tp/(tp+fn)) + (tn/(tn+fp)))/2\n",
    "    \n",
    "    return [round(accuracy*100,2),round(precision*100,2),round(recall*100,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Regression Model using Gradient Descent\n",
      "Iteration 1 Current Value of Error (Cost Function) based on updated values of beta parameters =  9.393293608662436\n",
      "Iteration 11 Current Value of Error (Cost Function) based on updated values of beta parameters =  4.499126468585671\n",
      "Iteration 21 Current Value of Error (Cost Function) based on updated values of beta parameters =  2.922044039962432\n",
      "Iteration 31 Current Value of Error (Cost Function) based on updated values of beta parameters =  2.173082109245744\n",
      "Iteration 41 Current Value of Error (Cost Function) based on updated values of beta parameters =  1.7360273990978219\n",
      "Iteration 51 Current Value of Error (Cost Function) based on updated values of beta parameters =  1.4487890322399581\n",
      "Iteration 61 Current Value of Error (Cost Function) based on updated values of beta parameters =  1.2450612097420737\n",
      "Iteration 71 Current Value of Error (Cost Function) based on updated values of beta parameters =  1.0927364559279025\n",
      "Iteration 81 Current Value of Error (Cost Function) based on updated values of beta parameters =  0.9743619967301627\n",
      "Iteration 91 Current Value of Error (Cost Function) based on updated values of beta parameters =  0.8796207493457944\n",
      "Training Interrupted as Maximum number of iterations were crossed.\n",
      "\n",
      "\n",
      "Final Values for Beta Parameters are (from beta_0 to beta_4) : [-0.6539124875804105, -0.9550378946465553, 0.5573476027764889, -1.0621091630209138, -1.0163336774276517]\n",
      "Accuracy: 98.52 Precision: 98.89 Recall: 97.87\n"
     ]
    }
   ],
   "source": [
    "X,Y,mean,std = get_data(\"../Data/data.csv\")\n",
    "\n",
    "# Training Data Input Variables\n",
    "train_X = np.concatenate((X[:,0:5],X[:,50:55],X[:,100:105]),1)\n",
    "# Testing Data Input Variables\n",
    "test_X = np.concatenate((X[:,5:50],X[:,55:100],X[:,105:150]),1)\n",
    "\n",
    "# Training Data Output Variable\n",
    "train_Y = np.concatenate((Y[0:5],Y[50:55],Y[100:105]),0)\n",
    "# Testing Data Output Variable\n",
    "test_Y = np.concatenate((Y[5:50],Y[55:100],Y[105:150]),0)\n",
    "\n",
    "\n",
    "################################################\n",
    "## Model Training (or coefficient estimation) ##\n",
    "################################################\n",
    "# Using our gradient descent function we estimate coefficients of our regression line. The gradient descent function \n",
    "# returns a list of coefficients\n",
    "\n",
    "# coefficients = [beta_0,beta_1,beta_2,beta_3,beta_4]\n",
    "coefficients = gradientDescentAlgorithm(train_X,train_Y,0.1)\n",
    "\n",
    "########################\n",
    "## Making Predictions ##\n",
    "########################\n",
    "\n",
    "# Using our predict function and the coefficients given by our gradient descent function we can now predict the time it will take\n",
    "# for the next eruption.\n",
    "print (\"Final Values for Beta Parameters are (from beta_0 to beta_4) :\",coefficients)\n",
    "\n",
    "pred_Y = []\n",
    "for i in range(0,np.transpose(test_X).shape[0]):\n",
    "    probability = predict_probability(coefficients,[np.transpose(test_X)[i]])\n",
    "    pred_Y.append(classify(probability))\n",
    "    \n",
    "pred_Y = np.array(pred_Y)\n",
    "    \n",
    "accuracy,precision,recall = evaluate_performance(test_Y,pred_Y)\n",
    "\n",
    "print (\"Accuracy:\",accuracy,\"Precision:\",precision,\"Recall:\",recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 15)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
